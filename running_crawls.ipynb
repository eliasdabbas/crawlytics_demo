{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5c349ce-03ba-4740-b955-8bdf093de7e0",
   "metadata": {},
   "source": [
    "# Get data on the currently running crawlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bda8337-453e-4e25-832e-268ea584cafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from subprocess import run\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "run = partial(run, text=True, capture_output=True)\n",
    "\n",
    "\n",
    "def running_crawls():\n",
    "    \"\"\"Get details of currently running spiders.\n",
    "\n",
    "    Get a DataFrame showing the following details:\n",
    "\n",
    "    * pid: Process ID. Use this to identify (or stop) the spider that you want.\n",
    "    * started: The time when this spider has started.\n",
    "    * elapsed: The elapsed time since the spider started.\n",
    "    * %mem: The percentage of memory that this spider is consuming.\n",
    "    * %cpu: The percentage of CPU that this spider is consuming.\n",
    "    * args: The full command that was used to start this spider. Use this to identify\n",
    "      the spider(s) that you want to know about.\n",
    "    * output_file: The path to the output file for each running crawl job.\n",
    "    * crawled_urls: The current number of lines in ``output_file``.\n",
    "    \"\"\"\n",
    "    ps = run([\"ps\", \"xo\", \"pid,start,etime,%mem,%cpu,args\"])\n",
    "    ps_stdout = ps.stdout.splitlines()\n",
    "    df = pd.DataFrame(\n",
    "        [line.split(maxsplit=5) for line in ps_stdout[1:]], columns=ps_stdout[0].split()\n",
    "    )\n",
    "    df[\"output_file\"] = df[\"ARGS\"].str.extract(r\"-o (.*?\\.jl)\")[0]\n",
    "    df_subset = df[df[\"ARGS\"].str.contains(\"scrapy runspider\")].reset_index(drop=True)\n",
    "    if df_subset.empty:\n",
    "        return pd.DataFrame()\n",
    "    crawled_lines = run([\"wc\", \"-l\"] + df[\"output_file\"].str.cat(sep=\" \").split())\n",
    "    crawl_urls = [\n",
    "        int(line.strip().split()[0]) for line in crawled_lines.stdout.splitlines()\n",
    "    ]\n",
    "    crawl_urls = crawl_urls[: min(len(crawl_urls), len(df_subset))]\n",
    "    df_subset[\"crawled_urls\"] = crawl_urls\n",
    "    df_subset.columns = df_subset.columns.str.lower()\n",
    "    return df_subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71bfc5c1-6379-427d-ba45-0909d9122b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>started</th>\n",
       "      <th>elapsed</th>\n",
       "      <th>%mem</th>\n",
       "      <th>%cpu</th>\n",
       "      <th>args</th>\n",
       "      <th>output_file</th>\n",
       "      <th>crawled_urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51735</td>\n",
       "      <td>3:15PM</td>\n",
       "      <td>19:01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/Library/Frameworks/Python.framework/Versions/...</td>\n",
       "      <td>/Users/me/Desktop/temp/time_crawl.jl</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51808</td>\n",
       "      <td>3:15PM</td>\n",
       "      <td>18:41</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/Library/Frameworks/Python.framework/Versions/...</td>\n",
       "      <td>/Users/me/Desktop/temp/nytimes_crawl.jl</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51899</td>\n",
       "      <td>3:16PM</td>\n",
       "      <td>18:08</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/Library/Frameworks/Python.framework/Versions/...</td>\n",
       "      <td>/Users/me/Desktop/temp/shopify_crawl.jl</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51975</td>\n",
       "      <td>3:16PM</td>\n",
       "      <td>17:53</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/Library/Frameworks/Python.framework/Versions/...</td>\n",
       "      <td>/Users/me/Desktop/temp/amazon_crawl.jl</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pid started elapsed %mem %cpu  \\\n",
       "0  51735  3:15PM   19:01  0.5  0.0   \n",
       "1  51808  3:15PM   18:41  1.2  0.0   \n",
       "2  51899  3:16PM   18:08  1.4  0.0   \n",
       "3  51975  3:16PM   17:53  0.7  0.0   \n",
       "\n",
       "                                                args  \\\n",
       "0  /Library/Frameworks/Python.framework/Versions/...   \n",
       "1  /Library/Frameworks/Python.framework/Versions/...   \n",
       "2  /Library/Frameworks/Python.framework/Versions/...   \n",
       "3  /Library/Frameworks/Python.framework/Versions/...   \n",
       "\n",
       "                               output_file  crawled_urls  \n",
       "0     /Users/me/Desktop/temp/time_crawl.jl            87  \n",
       "1  /Users/me/Desktop/temp/nytimes_crawl.jl            96  \n",
       "2  /Users/me/Desktop/temp/shopify_crawl.jl            80  \n",
       "3   /Users/me/Desktop/temp/amazon_crawl.jl            75  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running = running_crawls()\n",
    "running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16b01585-e9fa-4a89-b374-b266ab75937c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Library/Frameworks/Python.framework/Versions/3.12/Resources/Python.app/Contents/MacOS/Python',\n",
       " '/Users/me/venv312/bin/scrapy',\n",
       " 'runspider',\n",
       " '/Users/me/venv312/lib/python3.12/site-packages/advertools/spider.py',\n",
       " '-a',\n",
       " 'url_list=https://nytimes.com/',\n",
       " '-a',\n",
       " 'allowed_domains=nytimes.com',\n",
       " '-a',\n",
       " 'follow_links=True',\n",
       " '-a',\n",
       " 'exclude_url_params=None',\n",
       " '-a',\n",
       " 'include_url_params=None',\n",
       " '-a',\n",
       " 'exclude_url_regex=None',\n",
       " '-a',\n",
       " 'include_url_regex=None',\n",
       " '-a',\n",
       " 'css_selectors=None',\n",
       " '-a',\n",
       " 'xpath_selectors=None',\n",
       " '-o',\n",
       " '/Users/me/Desktop/temp/nytimes_crawl.jl',\n",
       " '-s',\n",
       " 'DOWNLOAD_DELAY=20',\n",
       " '-s',\n",
       " 'CLOSESPIDER_PAGECOUNT=200']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running['args'][1].split()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
